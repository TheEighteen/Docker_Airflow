# ğŸš€ ETL Data Pipeline with Apache Airflow & Docker

This project demonstrates a containerized **ETL (Extract, Transform, Load)** pipeline using **Apache Airflow**, orchestrated via **Docker**.

---

## ğŸ“Œ Features

- â›ï¸ **Extract**: Reads `input.csv` with transaction data.
- ğŸ”„ **Transform**: Filters rows with `amount > 100`.
- ğŸ“¤ **Load**: Saves the filtered data to `output/output.csv`.
- ğŸ“§ **Email Notification**: Sends an email with the result file attached after the DAG completes.
- ğŸ” **Scheduled** to run daily via Airflow's scheduler.
- ğŸ³ **Dockerized** for portable, reproducible setup.

---

## ğŸ› ï¸ Tech Stack

- **Python**
- **Apache Airflow**
- **Docker / Docker Compose**
- **Pandas** for data manipulation

---

## ğŸ“‚ Project Structure

```bash
.
â”œâ”€â”€ dags/
â”‚   â””â”€â”€ etl_data_pipeline.py        # Airflow DAG definition
â”œâ”€â”€ input/
â”‚   â””â”€â”€ input.csv                   # Sample input data
â”œâ”€â”€ output/
â”‚   â””â”€â”€ output.csv                  # Output after ETL
â”œâ”€â”€ docker-compose.yml             # Airflow + Docker config
â””â”€â”€ README.md                       # You're here
